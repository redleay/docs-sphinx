GPT


在Transform被提出之后的几年内，出现了一些变种，比如仅编码器的BERT、仅解码器的GPT和编码器+解码器的T5。而真正大杀四方的是只有openAI的GPT。

ChatGPT基于Transformer提取了解码器部分并做了一些改造，以下是chatGPT2.0及之后版本的**架构**。

ChatGPT的架构与Transformer解码器相似，只是个别组件做了位置调整。

# 训练过程

1. 预训练（Pre-training）：通过大量的语料、多次的训练，学习词汇、语法和语义等语言特征，让词信息在高维空间中找到合适的位置，形成深度神经网络结构，得到一个可以生成文本的基座模型。

2. 有监督微调（Supervised Fine Tuning，SFT）：将预训练好的模型，加上有标签的数据集，再次训模型。不同的应用场景，其训练目的也不同，通用大模型的微调是为了让模型表达更清晰、更能举一反三，行业大模型的微调是为了让大模型更能理解行业知识解决行业问题。
3. 基于人类反馈的强化学习（Reinforcement Learning from Human Feedback，RLHF）：输入高质量、有目的性的数据集，人工给大模型输出的多个回答进行排序，训练一个奖励模型，利用评分作为反馈，对大模型进行强化学习。RLHF一般会分成两个步骤：
    - 奖励建模（reward modelling，RM）：给模型输出的多个答案进行排序评分，训练出一个奖励模型。
    - 强化学习（reinforcement learning，RL）：将奖励模型中得到的权重，结合SFT有监督微调，强化大模型学习效果。

## 预训练

预训练总体上分成两个步骤，词嵌入（Embedding）和特征提取。
词嵌入是将文字语言转成数学语言的过程，为了让计算机能进行数学运算；特征提取是自然语言理解的过程，将语言信息和规律“记录”到神经网络中。

### 词嵌入

词嵌入（Embedding）是将词转化成向量的过程，其作用是把文本转化成数学的形式，让计算机可以理解和计算。训练后的每个词在向量空间中的相对位置，表示了词之间在某些方面的相关性，一定程度上反映了他们在物理世界的语义关系。

#### 分词（tokenization）：

原始的输入文本经过tokenization之后被分解成一系列token。token可以是一个字、一个词、一个符号，也可以是一个英文单词的后缀。
在大语言模型训练之前，会先维护一个token词表，语料输入的时候根据这个查找表找到对应的token编码。
具体如何划分，openAI用一种叫做Byte Pair Encoding（BPE）的子词划分方法。BPE可以根据语料库中出现频率最高的字节对（byte pair）来合并字节，从而生成新的字节，比如词根也可以独立成为一个token。不论如何，tokennization的过程是为了让神经网络更好地理解人类的语言结构。

#### 词嵌入（Embedding）

将上面的得到的向量矩阵，加上每个token的位置编码（向量）。
在Transformer模型中叫Positional Encoding，此过程是为了记住每个token在句子中的位置

Transformer的位置编码用正余弦函数计算，值的大小控制在-1~1之间，对原来的词向量影响就更小了。
位置编码在Transformer论文中用sin和cos来计算，偶数用sin，奇数用cos

公式

### Self-Attention特征提取

#### 计算自注意力

将X分别与Wq、Wk和Wv三个权重矩阵进行点乘，得出Q（查询向量）、K（键向量）、V（值向量）。Wq、Wk和Wv一开始可能是个随机数，在后期训练中不断调整优化。

将Q向量与旋转90°之后的K向量求进行点积，得出新的向量矩阵A，相当于每个token与所有token之间建立关系，表示每个token对所有token的“关注度”程度。相乘之后的结果越大，其关注度越高。


$$
Q = W_q X \newline
K = W_k X \newline
V = W_v X \newline
A = Q K^T \newline
$$


#### Layer Normalization

此过程是为了将参数进行缩放，避免后期训练过程中出现梯度消失或梯度爆炸的问题

$$
A_d = \frac {A} {\sqrt{d_k}} = \frac {Q K^T} {\sqrt{d_k}}
$$

#### 调整输出权重随机性

为了让模型在输出答案的时候，让每次回答都有所不同，而不是单一的回复，可以通过“温度”（temperature）参数来控制softmax函数输出的随机性，温度越高，随机性就越大

$$
softmax(A_d)
$$

#### 计算注意力权重

计算每个词每个维度下与其他所有词在所有维度下的关注度（解码器的注意力增加了掩码，在计算关注度时要把未输出的词挡住）

$$
Out = A_d V
$$

### 推理

模型的输出主要有3个步骤：

1. 文本输入：将输入的文本向量化嵌入，与训练好的权重矩阵Wq、Wk、Wv进行一系列线性变换运算，给到前馈神经网络隐藏层进行输出预测。
2. 生成第一个词：经过相关性计算，找到第一个概率最高的词，进行解码输出。为了让输出更加有创造力，会加入“温度”的参数设置，让输出结果有一定的随机性。
3. 迭代生成：一旦生成了第一个词，解码器就会将这个词添加到它的输入中，结合输入文本继续预测下一个词。这个过程会一直持续，直到生成了句子结束标记<EOS>或者达到预设的最大句子长度。

所有的大语言模型都有最大的上下文窗口token限制，为了让对话更持久，各大产商都在想方设法突破限制，Kimi已经达到了20万字

## 有监督微调SFT

做法大体就是给模型输入人类标注好的高质量语料，对模型输出结果进行调整，以改变内部token的参数。

基于不同的目标，可以采用不同的微调方法或者混合使用，微调方法从大类上分为全参微调和部分微调。
- 全参微调对整个预训练模型所有内部参数进行微调。全参微调成本较高，且过渡微调，容易导致模型过拟合。
- 部分微调只更新顶层或少数几层网络，保持预训练模型的底层参数不变。部分微调成本较低，在保持模型泛化能力的同时还加强了对特定场景任务的适应。

目前已有的微调技术有:
- Adapter Tuning（适配器微调）
- Prefix Tuning（前缀微调）
- Prompt Tuning（提示词指令微调）
- P-Tuning（连续提示微调）
- LoRA（低秩适应微调）
- QLoRA（量化低秩适应微调）

总体来说，不同微调技术，解决问题的针对性不同，大体可以归纳为以下几种：

- 加入更高质量的语料，强化模型语言表达能力、泛化能力。
- 加入行业知识，更新神经网络某些层，让模型在某些领域的任务能力更佳。
- 对输入的提示词进行丰富和任务拆解，再微调训练模型，以适应用户侧各式各样的输入信息，且不至于让模型过拟合。


## 基于人类反馈的强化学习

为了让模型表现更佳、品性兼优，比如考核模型的三观是否端正，还需要经过一次考核，就是基于人类反馈的强化学习。

### 训练奖励模型

大体做法是，给模型输入特定的问题，通过人工给所答案进行打分（这些语料可以覆盖心理、教育、法理、社会学等各个领域），好比老师给学生提交的作业打分一样。

不同的是，模型对于同一个问题，输出多个答案，然后给答案进行排序。

这些问题、答案和评分，放在另一个模型中进行训练，得到一个“奖励模型”。（这一步还是需要人工的介入）

### 强化学习

当奖励模型训练好之后，由奖励模型给模型的回答进行打分，并反馈给模型进行微调。

利用奖励模型的好处是既保留模型的泛化能力，又对模型的回答往更好的方向引导，而不至于非对即错。

强化学习的方法其实也有很多种，基于人类反馈的强化学习（RLHF）已经被广泛认可，世界顶级的企业也在用这个方法，比如openAI、Meta。23年9月Google研究院也已经提出通过AI来代替人工给答案排序打分的方法RLAIF，大幅度提高训练。
感兴趣可以翻一下原论文：RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback

