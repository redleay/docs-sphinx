# 范数

## Frobenius范数

定义为矩阵的各项元素的平方的总和的开方：

$$
\left \| X \right \| _F = \sqrt {\sum_i \sum_j X^2_{i,j}}
$$

# 熵

## 信息熵

$$
H(P) = - \sum_{i} {p(x_i) \log p(x_i)}
$$

## 交叉熵

$$
H(p, q) = - \sum_{i} {p(x_i) \log q(x_i)}
$$

$p$ 和 $q$ 的交叉熵可以看做是，使用分布 $q(x)$ 表示目标分布 $p(x)$ 的困难程度。

## 相对熵

$$
D_{KL} (P \parallel Q) = \sum_{x \in X} P(x) \log \frac {P(x)} {Q(x)}
$$

用于衡量两个分布之间距离，但不对称，从不同方向计算会得到不同的距离，即

$$ D_{KL} (P \parallel Q) \ne D_{KL} (Q \parallel P) $$

# 散度

## KL散度

即相对熵，参考相对熵章节。

KL散度在GAN中非常容易造成模式崩塌，即生成数据的多样性不足。

## JS散度

在KL散度的基础上进行了修正，对两个方向KL散度取平均值，保证了对称性：

$$
D_{JS} ( P \parallel Q) = \frac 1 2 D_{KL} \left (P \parallel \frac {P+Q} {2} \right ) + \frac 1 2 D_{KL} \left (Q \parallel \frac {P+Q} {2} \right )
$$

JS散度存在一个严重的问题：两个分布不重叠时，JS散度为零，而在训练初期，两个分布非常大可能不重叠。

无论KL散度还是JS散度，直接用作loss时，都是难以训练的：由于分布只能通过取样计算，这个loss在每次迭代时都几乎为零。

# Gram矩阵

n维欧式空间中任意n个向量之间两两的内积所组成的矩阵，称为这k个向量的Gram矩阵，即格拉姆矩阵

$$
G = A^T A = \begin{bmatrix} a_1^T \\ a_2^T \\ \vdots \\ a_2^T \end{bmatrix} \begin{bmatrix} a_1 a_2 \cdots a_n \end{bmatrix}
$$

很明显，这是一个对称矩阵。

格拉姆矩阵可以看做feature之间的偏心协方差矩阵（即没有减去均值的协方差矩阵），在feature map中，每个数字都来自于一个特定滤波器在特定位置的卷积，因此每个数字代表一个特征的强度，而Gram计算的实际上是两两特征之间的相关性，哪两个特征是同时出现的，哪两个是此消彼长的等等。

格拉姆矩阵用于度量各个维度自己的特性以及各个维度之间的关系。内积之后得到的多尺度矩阵中，对角线元素提供了不同特征图各自的信息，其余元素提供了不同特征图之间的相关信息。这样一个矩阵，既能体现出有哪些特征，又能体现出不同特征间的紧密程度。 

