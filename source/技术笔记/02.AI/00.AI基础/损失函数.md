# 相关基础

## 范数

### Frobenius范数

定义为矩阵的各项元素的平方的总和的开方：

$$
\left \| X \right \| _F = \sqrt {\sum_i \sum_j X^2_{i,j}}
$$

## 熵

![descript](./损失函数/media/image2.svg){width="2.25in"
height="0.3958333333333333in"}

## 相对熵（KL散度）

用于衡量两个分布之间距离，但不对称，从不同方向计算会得到不同的距离：

![descript](./损失函数/media/image4.svg){width="2.75in" height="0.5in"}

KL散度在GAN中非常容易造成模式崩塌，即生成数据的多样性不足。

## 交叉熵

![descript](./损失函数/media/image6.svg){width="2.4166666666666665in"
height="0.3958333333333333in"}

p和q的交叉熵可以看做是，使用分布q(x) 表示目标分布p(x)的困难程度。

## JS散度

在KL散度的基础上进行了修正，对两个方向KL散度取平均值，保证了对称性：

![descript](./损失函数/media/image8.svg){width="4.739583333333333in"
height="0.3854166666666667in"}

JS散度存在一个严重的问题：两个分布不重叠时，JS散度为零，而在训练初期，两个分布非常大可能不重叠。

无论KL散度还是JS散度，直接用作loss时，都是难以训练的：由于分布只能通过取样计算，这个loss在每次迭代时都几乎为零。

## Gram矩阵

n维欧式空间中任意n个向量之间两两的内积所组成的矩阵，称为这k个向量的格拉姆矩阵(Gram matrix)

$$
G = A^T A = \begin{bmatrix} a_1^T \\ a_2^T \\ \vdots \\ a_2^T \end{bmatrix} \begin{bmatrix} a_1 a_2 \cdots a_n \end{bmatrix}
$$

很明显，这是一个对称矩阵。

格拉姆矩阵可以看做feature之间的偏心协方差矩阵（即没有减去均值的协方差矩阵），在feature map中，每个数字都来自于一个特定滤波器在特定位置的卷积，因此每个数字代表一个特征的强度，而Gram计算的实际上是两两特征之间的相关性，哪两个特征是同时出现的，哪两个是此消彼长的等等。

格拉姆矩阵用于度量各个维度自己的特性以及各个维度之间的关系。内积之后得到的多尺度矩阵中，对角线元素提供了不同特征图各自的信息，其余元素提供了不同特征图之间的相关信息。这样一个矩阵，既能体现出有哪些特征，又能体现出不同特征间的紧密程度。 

# Perceptual Loss

出处：李飞飞团队，发表于ECCV2016

论文：[Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)

参考文章：

<https://fengweiustc.github.io/paper-reading/2019/11/05/perceptualLoss/>

## 基本思想

在CV任务中，需要把输入图片转化成目标图片，这个目标图片不一定是精确的某张图，可能是指定的图像风格迁移、超分辨率等目标。
传统的pixel-wise loss追求像素级的值相等，然而并不是pixel-loss低，output和target就会差距小。
因此, 在侧重追求视觉效果的更高level的任务中, 以feature作为loss更能反映实际的优化方向。

## 创新意义

产生了一个非常重要的idea，那就是可以将CNN提取出的feature，作为目标函数的一部分，
通过比较待生成的图片经过CNN的feature值与目标图片经过CNN的feature值，
使得待生成的图片与目标图片在语义上更加相似(相对于Pixel级别的损失函数)。

## 系统框架

![descript](./损失函数/media/perceptual_loss_framework.png)

网络分为两部分，其中，Image Transfrom Net是待训练的网络，Loss Network(VGG-16)是预训练的模型，参数冻结固定不变。
使用预训练VGG-16模型，分别对GT和网络生成结果，提取特征，衡量高层特征之间的距离，即Perceptual Differences，使高层特征接近，
而VGG模型的高层特征能体现内容和全局结构，因此具有感知能力。

## 计算公式

分为两部分，Feature Reconstruction Loss和Style Reconstruction Loss。

首先，定义以下变量：
- $\phi$: VGG网络
- x: input image
- $\hat{y}$: output image
- $y$: target image
- $\phi_j(x)$: 输入$x$时$\phi$中第$j$层输出的feature
- C, H, W: feature的维度

### Feature Reconstruction Loss

定义为output feature和target feature之间的欧氏距离：

$$
\ell ^{\phi, j}_{feat} (\hat{y},y) = \frac {1} {C_j H_j W_j} \left \| \phi_j(\hat{y}) - \phi_j(y) \right \| _2^2
$$

### Style Reconstruction Loss

首先，定义一个Gram矩阵，若input image为$x$，对于$\phi$的第$j$层输出feature，其Gram矩阵$(c, c')$位置的值为：

$$
G^{\phi}_j (x)_{c,c'} = \frac {1} {C_j H_j W_j} \sum^{ H_j }_{h=1} \sum^{ W_j }_{w=1} \phi_j(x)_{h,w,c} \phi_j(x)_{h,w,c'}
$$

> 如果输入feature的维度是$C×H×W$, 该矩阵的维度是$C \times C$


若将$\phi _j(x)$重新reshape为$C_j \times H_j W_j$形状的矩阵$\psi$，则

$$
G^{\phi}_j (x) = \frac {\psi \psi^T} {C_j H_j W_j}
$$

则Style Reconstruction Loss定义为output feature和target feature的Gram矩阵的差值的Squared Frobenius Norm：

$$
\ell ^{\phi,j}_{style} (\hat{y}, y) = \left \| G^{\phi}_j (\hat{y}) - G^{\phi}_j (y) \right \| _F^2
$$

> 注：Gram矩阵的shape为$C \times C$，与$H$和$W$无关，即使output image和target image的shape不同, 依然可以计算Loss。

> 若将$\phi _j(x)$解释为在$H_j \times W_j$网格上的每个点的$C_j$维特征，且每个点为相互独立采样点，则$G_j^{\phi}(x)$与每个点的$C_j$维特征的非中心协方差成正比关系，因此捕获了关于哪些特征倾向于同步激活的信息。

## 优点

一般会在imagenet这种大数据量级上进行训练，特征非常general，
输出结果具有高频细节信息，
风格转移或者超分辨率中，速度快，GAN中，收敛效果好，
收敛速度快，因为回传导数时，相比于pixel-pixel差异， 回传分布更具有普适性。

# GAN Loss

原本是GAN里面的对抗loss，现在在超分任务中也被常规采用，有很多种形式，有时也需要和梯度正则一起用。

有利于生成重复纹理，但可能产生伪影，可用Pixel Loss约束

![descript](./损失函数/media/image16.svg)

一切损失计算都是在判别器D输出处产生的，而D的输出一般是true/fake的判断，所以整体上采用的是二进制交叉熵函数。

左边包含两部分minG和maxD：

- maxD：GAN训练一般是先保持G不变训练D。D的训练目标是正确区分true/fake，如果以1/0代表true/fake，则对第一项求和∑，因为输入来自真实数据所以期望D(x)趋近于1，即第一项越大越好，同理第二项求和∑的输入来自G生成数据所以期望D(G(z))趋近于0，即第二项越大越好，所以期望整体Loss越大越好，这就是maxD的含义。

- minD：保持D不变训练G，此时只有第二项求和∑有用，D的所有输入都是G生成的fake数据，但希望G能迷惑D，D(G(z))趋近于1，即第二项求和∑越小越好，这就是minG的含义。

# Charbonnier Loss

L1 Loss的改良版，加了一个正则项$\epsilon$，在零点附近更加平滑：
- 接近零点的值的梯度由于$\epsilon$的存在，梯度不会太小，避免梯度消失
- 远离零点的值的梯度由于开方，梯度也不会太大，避免梯度爆炸

$$
L_{pixel\_charbonnier} = \frac {1} {hwc} \sum_{i,j,k} \sqrt{\left ( \hat{I}_{i,j,k} - I_{i,j,k} \right )^2 + \epsilon ^2}
$$

# Wasserstein Loss

# Edge Loss

如果要强调边缘可以使用Edge Loss，基本原理是利用sobel等边缘检测算子检测GT的边缘作为mask，计算Edge Loss时只针对mask标记为边缘区域的像素计算Loss。

# Color Loss

有2种思路：
1. 角误差：取GT和网络输出之间的RGB 3D Vector之间的角度差，在白平衡任务中常用
2. 使用高斯模糊核，把图像的纹理信息模糊掉了然后计算模糊后的L2 Loss，参考`DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks`这篇文章
