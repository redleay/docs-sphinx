# Perceptual Loss

出处：李飞飞团队，发表于ECCV2016

论文：[Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)

参考文章：

<https://fengweiustc.github.io/paper-reading/2019/11/05/perceptualLoss/>

## 基本思想

在CV任务中，需要把输入图片转化成目标图片，这个目标图片不一定是精确的某张图，可能是指定的图像风格迁移、超分辨率等目标。
传统的pixel-wise loss追求像素级的值相等，然而并不是pixel-loss低，output和target就会差距小。
因此, 在侧重追求视觉效果的更高level的任务中, 以feature作为loss更能反映实际的优化方向。

## 创新意义

产生了一个非常重要的idea，那就是可以将CNN提取出的feature，作为目标函数的一部分，
通过比较待生成的图片经过CNN的feature值与目标图片经过CNN的feature值，
使得待生成的图片与目标图片在语义上更加相似(相对于Pixel级别的损失函数)。

## 系统框架

![descript](./损失函数/media/perceptual_loss_framework.png)

网络分为两部分，其中，Image Transfrom Net是待训练的网络，Loss Network(VGG-16)是预训练的模型，参数冻结固定不变。
使用预训练VGG-16模型，分别对GT和网络生成结果，提取特征，衡量高层特征之间的距离，即Perceptual Differences，使高层特征接近，
而VGG模型的高层特征能体现内容和全局结构，因此具有感知能力。

## 计算公式

分为两部分，Feature Reconstruction Loss和Style Reconstruction Loss。

首先，定义以下变量：
- $\phi$: VGG网络
- x: input image
- $\hat{y}$: output image
- $y$: target image
- $\phi_j(x)$: 输入$x$时$\phi$中第$j$层输出的feature
- C, H, W: feature的维度

### Feature Reconstruction Loss

定义为output feature和target feature之间的欧氏距离：

$$
\ell ^{\phi, j}_{feat} (\hat{y},y) = \frac {1} {C_j H_j W_j} \left \| \phi_j(\hat{y}) - \phi_j(y) \right \| _2^2
$$

### Style Reconstruction Loss

首先，定义一个Gram矩阵，若input image为$x$，对于$\phi$的第$j$层输出feature，其Gram矩阵$(c, c')$位置的值为：

$$
G^{\phi}_j (x)_{c,c'} = \frac {1} {C_j H_j W_j} \sum^{ H_j }_{h=1} \sum^{ W_j }_{w=1} \phi_j(x)_{h,w,c} \phi_j(x)_{h,w,c'}
$$

> 如果输入feature的维度是$C×H×W$, 该矩阵的维度是$C \times C$


若将$\phi _j(x)$重新reshape为$C_j \times H_j W_j$形状的矩阵$\psi$，则

$$
G^{\phi}_j (x) = \frac {\psi \psi^T} {C_j H_j W_j}
$$

则Style Reconstruction Loss定义为output feature和target feature的Gram矩阵的差值的Squared Frobenius Norm：

$$
\ell ^{\phi,j}_{style} (\hat{y}, y) = \left \| G^{\phi}_j (\hat{y}) - G^{\phi}_j (y) \right \| _F^2
$$

> 注：Gram矩阵的shape为$C \times C$，与$H$和$W$无关，即使output image和target image的shape不同, 依然可以计算Loss。

> 若将$\phi _j(x)$解释为在$H_j \times W_j$网格上的每个点的$C_j$维特征，且每个点为相互独立采样点，则$G_j^{\phi}(x)$与每个点的$C_j$维特征的非中心协方差成正比关系，因此捕获了关于哪些特征倾向于同步激活的信息。

## 优点

- 一般会在`ImageNet`这种大数据量级上进行训练，特征非常general
- 输出结果具有高频细节信息
- 风格转移或者超分辨率中，速度快，GAN中，收敛效果好
- 收敛速度快，因为回传导数时，相比于pixel-pixel差异，回传分布更具有普适性

# Charbonnier Loss

L1 Loss的改良版，加了一个正则项$\epsilon$，在零点附近更加平滑：
- 接近零点的值的梯度由于$\epsilon$的存在，梯度不会太小，避免梯度消失
- 远离零点的值的梯度由于开方，梯度也不会太大，避免梯度爆炸

$$
L_{pixel\_charbonnier} = \frac {1} {hwc} \sum_{i,j,k} \sqrt{\left ( \hat{I}_{i,j,k} - I_{i,j,k} \right )^2 + \epsilon ^2}
$$

# Edge Loss

如果要强调边缘可以使用Edge Loss，基本原理是利用sobel等边缘检测算子检测GT的边缘作为mask，计算Edge Loss时只针对mask标记为边缘区域的像素计算Loss。

# Color Loss

有2种思路：
1. 角误差：取GT和网络输出之间的RGB 3D Vector之间的角度差，在白平衡任务中常用
2. 使用高斯模糊核，把图像的纹理信息模糊掉了然后计算模糊后的L2 Loss，参考`DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks`这篇文章

# GAN Loss

出自GAN里面的对抗Loss，具体参考GAN相关章节。

# Wasserstein Loss

GAN Loss的改进版本，具体参考GAN当中的WGAN相关章节。

