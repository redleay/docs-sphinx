#### Epoch 迭代次数

当一个完整的数据集经过神经网络一次，并返回一次，这个过程称为一个epoch。

- 即：1个epoch = 使用训练集中的全部样本训练一次 =
  所有训练样本的一个正向传递 &
  一个反向传递每一个epoch都需打乱数据的顺序，以使网络受到的调整更具有多样性。

#### Batch

当数据集很大时，对于每个
epoch，很难将所有的数据集一次读入到内存中，这就需要将数据集分为几次读入，每次称为一个
batch。

#### Batch_Size 批大小 批尺寸

一个 Batch
中样本的数量。在深度学习中，一般采用SGD训练，即每次在训练集中取
Batch_Size 个样本训练。

- 每次会选取 Batch_Size
  个样本，分别代入网络，算出它们分别对应的参数调整值，然后将所有调整值取平均，作为最后的调整值，以此调整网络的参数。

  - 如果 Batch_Size
    很大（例如和全部样本的个数一样），那么可保证得到的调整值很稳定，是最能让全体样本受益的改变。Batch_Size
    过大，不同batch的梯度方向没有任何变化，容易陷入局部极小值。

  - 如果 Batch_Size 较小（例如
    Batch_Size=1），那么得到的调整值有一定的随机性，因为对于某个样本最有效的调整，对于另一个样本却不一定最有效（就像对于识别某张黑猫图像最有效的调整，不一定对于识别另一张白猫图像最有效）。Batch_Size
    过小，花费时间多，同时梯度震荡严重，不利于收敛

  - 合适的 batchsize 对于网络的训练很重要Batch_Size
    增大到某个时候，达到时间上的最优

    - 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size
      增大到某些时候，达到最终收敛精度上的最优

- 为什么需要有 Batch_Size 这个参数？

  - Batch_Size
    的选择，首先决定的是**下降的方向**。如果数据集比较小，完全可以采用全数据集（Full
    Batch Learning）的形式，这样做至少有 2 个好处：①
    由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。②
    由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full
    Batch Learning 可以使用 Rprop
    方法只基于梯度符号并且针对性单独更新各权值。

  - 而对于更大的数据集，以上 2 个好处又变成了 2 个坏处：①
    随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。②
    以 Rprop 的方式迭代，会由于各个 Batch
    之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来
    RMSProp 的妥协方案。

- 在合理范围内，增大 Batch_Size 的好处？

  - 内存利用率提高了，大矩阵乘法的并行化效率提高。

  - 跑完一次
    epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。

  - 在一定范围内，一般来说 Batch_Size
    越大，其确定的下降方向越准，引起训练震荡越小。

- 盲目增大 Batch_Size 的坏处？

  - 内存利用率提高了，但是内存容量可能撑不住了。

  - 跑完一次
    epoch（全数据集）所需的迭代次数减少，但要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。

  - Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

#### iteration 迭代

1个 iteration = 使用 Batch_Size 个样本样本训练一次 = 一个正向通过 &
一个反向通过

**e.g.** 训练集有1000个样本数据，batchsize =
10，那么，训练完一次整个样本集需要：1000/10 = 100次iteration、1次epoch。

深度学习的优化算法其实就是梯度下降。每次的参数更新有三种方式：

#### Batch Gradient Decent 批梯度下降 BGD

遍历全部数据集（Batch =
1），算一次损失函数，然后算函数对各个参数的梯度，更新梯度。

#### 性能相对较好，但是计算量大，速度慢，不支持在线学习。Stochastic Gradient Decent 随机梯度下降 SGD

每次只选取1个样本（Batch_Size = 1），然后求梯度更新参数。

#### 计算速度快，但是收敛性能不好，容易在最优点附近震荡，hit不到最优点。两次参数的更新也有可能相互抵消，造成目标函数振荡得比较剧烈。Mini-batch Gradient Decent 小批梯度下降

综合上述两种方法的折中手段。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。

把batch分成小batch，按批在小batch上更新参数。

一方面，因为小batch的样本数与整个数据集相比小了很多，计算量不大；另一方面，一个小batch中的一组数据共同决定了本次梯度的方向，下降就不容易跑偏，减少了随机性。
