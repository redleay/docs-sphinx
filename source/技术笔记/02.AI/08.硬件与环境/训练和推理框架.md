训练和推理框架

vLLM是一种解决当前GPU资源限制的方案，它是一个快速且易于使用的LLM推理和服务库。它在服务吞吐量方面是最先进的框架，同时开创性地使用PagedAttention高效管理注意力键和值内存，并且支持多种量化模型等，不仅如此它还与Hugging
Face模型无缝衔接，对分布式推理并行支持兼容OpenAI的API服务器，还支持了上百种开源模型。

vLLM由加州大学伯克利分校开发，2023年期间在Chatbot Arena和Vicuna
Demo进行了部署。即使像LMSYS这样的小型研究团队计算资源有限，也能负担得起LLM服务的核心技术。随后Koala和
LLaMA等越来越多受欢迎的模型都开始使用vLLM提供服务。

vLLM采用了分页注意力算法（PagedAttention），这是一个新型注意力算法，可有效管理注意力键和值。配备分页注意力算法的vLLM重新定义了LLM服务的新技术水平：它的吞吐量比HuggingFace
Transformers高出24倍，而且无需更改任何模型架构。
