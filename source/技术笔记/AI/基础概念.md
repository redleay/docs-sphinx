# AI基础概念

## 分类

按任务目标，可划分为分类、回归、聚类、降维四个类型；  
按训练方法，可划分为有监督学习、半监督学习、无监督学习（自监督学习）、强化学习等类别；  
按应用领域，可划分为计算机视觉（CV）、自然语言处理（NLP）两个方向。  

## 应用

按目前的行业现状，CV的应用场景可划分为High-Level和Low-Level两大场景，其中High-Level场景有
- 物体检测
- 物体分类
- 物体识别
- 物体跟踪
- 语义分割

Low-Level场景主要有：
- 降噪
- 超分
- 超帧
- HDR合成
- 画质恢复
- 色调增强
- 色彩增强
- 风格转换

## 性能参数

查准率：准确率（Precision）:  
查全率，召回率（Recall）:   
交并比 - Intersection Over Union (IOU)  
P-R曲线：  
mAP：  

## 机器学习主要技术

PCA主成分分析  
SVM  

## 深度学习基础概念


### 网络结构组成

损失函数  
激活函数  

假设CNN网络中某一层的输入数据维度为$ic \times ih \times iw$，输出数据维度为$oc \times oh \times ow$，则CNN的卷积核维度为$oc \times ic \times kh \times kw$，其中，

- $ic$是输入的通道数，如，RGB图像的通道为3
- $iw$和$ih$是输入的宽高
- $oc$是输出的通道数
- $ow$和$oh$是输出数据中每个通道的尺寸，可代表输出空间内容，或其他无具体物理意义的特征
- $kw$和$kh$是卷积核的尺寸
- $oc$决定有多少个不同的卷积核，每个卷积核的维度为$ic \times kw \times kh$，即卷积核需要跨越输入数据的所有通道
- 卷积核即为神经元
- $iw$和$kw$以及pading的大小决定$ow$，若$kw$为奇数，计算公式为$ow = iw + 2 * pad - (kw - 1)$，$oh$同理

权值共享：在同一个卷积层中，对于所有的像素，卷积核的参数相同  

### 训练迭代过程

深度学习中最基本的思想为梯度下降，反向传播减小误差优化参数。

前向传播  
反向传播  
权值更新  

训练的数量概念：
1. batchsize：一个迭代批次样本数量的大小，即每次迭代使用的样本数量
2. iteration：1个iteration是指一次迭代过程，即完成一次前向和反向的整个过程
3. epoch：1个epoch是指训练集中的全部样本都训练了一次，训练集中所有的样本都被迭代了一次就是完成了一个epoch，通常将的几个epoch就是指训练集中的所有样本被迭代了几次

参数优化涉及到Optimizer和Scheduler，两者同时作用，共同决定训练过程中的每次iteration的参数调整幅度。  
Optimizer/Scheduler与iteration/epoch的关系：
1. Optimizer：一般以iteration为单位更新模型参数，训练过程中每增加一次iteration，根据反向传播和梯度下降法更新模型的参数
2. Scheduler：一般以epoch为单位调整学习率，训练过程中每增加一个或多个epoch，学习率逐渐变小

Optimizer：

根据梯度更新模型参数，有梯度下降法（GD, BGD, SGD）、动量优化法（Momentum, NAG）、自适应学习率优化法（AdaGrad, RMSProp, AdaDelta, Adam）三大类算法。
1. Batch Gradient Descent：每次迭代（完成一次前向和反向运算）会计算训练数据集中所有的样本，在深度学习中训练样本数量通常达十万、百万甚至更多，这样一次迭代遍历所有的样本显然很慢
2. Stochastic Gradient Descent：每次迭代只使用训练集中的一个样本，即一次前向仅使用1个样本计算损失函数，然后计算梯度更新参数，这种方式虽然一次迭代速度很快，但是每次仅使用1个样本计算损失函数，容易受单个样本的质量好坏干扰，较难找到优化的最优点
3. Mini-Batch Gradient Decent（小批量梯度下降）：这种方式介于上面两种方法之间，一次迭代使用小批量的数据，既能够防止一次迭代仅有1个样本带来的收敛性较差的问题，同时，每次迭代只是用了小批量样本，不会像Batch gradient descent那样单次迭代耗时过长。

这篇文章[《各种优化器Optimizer的总结与比较》](https://blog.csdn.net/weixin_40170902/article/details/80092628)详细对比了各种优化器的原理和特点。

Scheduler：  
调整学习率


## 重要技术模块

### 卷积

空洞卷积：可扩大模型的感受野

可分离卷积：

可变形卷积：

反卷积（Transposed Convolution）：又称转置卷积，可起到上采样(Upsample)的作用，一种特殊的正向卷积，先按照一定的比例通过补 [公式] 来扩大输入图像的尺寸，接着旋转卷积核，再进行正向卷积。只能恢复尺寸，不能恢复数值。

1x1卷积：降维，综合所有channel的信息，提取重要信息

### 多尺度结构

高斯金字塔：可缩小图像，保留重要的低频信息。  

拉普拉斯金字塔：可逐层恢复图像的原始细节信息。

### 注意力机制

一种加权方法，有对齐数据、提高特征捕获效率的作用，通常与Encoder-Decoder框架结合使用，有相似度估计、数值转换、加权求和3个阶段。若相似度估计的对象是输入元素和输出元素，则为常规Attention；若估计的是输入元素之间的相似度，则为Self-Attention。数值转换的作用通常为归一化。

SEBlock:

PSA:

CBAM:

### 轻量化

知识蒸馏：可降低模型的复杂度

量化：

## 相关数学

范数

正定矩阵

二次型

最小二乘法

协方差矩阵

特征值：对一个矩阵A，存在一个向量v，使得Av = lambda \* v，即在向量v上，A的作用与lambda等同。lambda称为特征值，v称为特征向量

奇异值分解：缩写SVD

KL散度:

JS散度：

交叉熵：




